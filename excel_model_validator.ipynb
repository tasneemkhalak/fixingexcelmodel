{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03818e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Excel Model Validator - Phase 2: Testing and Validation\n",
    "Comprehensive testing framework to ensure model integrity after optimization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Container for validation test results\"\"\"\n",
    "    test_name: str\n",
    "    passed: bool\n",
    "    actual_value: Any\n",
    "    expected_value: Any = None\n",
    "    tolerance: float = 0.001\n",
    "    message: str = \"\"\n",
    "    execution_time: float = 0.0\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'test_name': self.test_name,\n",
    "            'passed': self.passed,\n",
    "            'actual_value': self.actual_value,\n",
    "            'expected_value': self.expected_value,\n",
    "            'tolerance': self.tolerance,\n",
    "            'message': self.message,\n",
    "            'execution_time': round(self.execution_time, 4)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fafb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcelModelValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive Excel Model Validation and Testing Framework\n",
    "    Ensures model integrity before and after optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, excel_file_path: str, baseline_data: Optional[Dict] = None):\n",
    "        self.excel_file_path = Path(excel_file_path)\n",
    "        self.baseline_data = baseline_data\n",
    "        self.app = None\n",
    "        self.wb = None\n",
    "        \n",
    "        # Test results tracking\n",
    "        self.validation_results = []\n",
    "        self.performance_metrics = {}\n",
    "        self.functional_tests = {}\n",
    "        \n",
    "        # Test configuration\n",
    "        self.test_config = {\n",
    "            'numerical_tolerance': 0.001,\n",
    "            'performance_tolerance': 2.0,  # 2x slower is acceptable\n",
    "            'required_macros': ['UpdateFinancialModel', 'SolveForLeasePayment', 'ResetModel'],\n",
    "            'critical_cells': {\n",
    "                'Inputs': ['B4', 'B5', 'B13', 'B14'],  # Capex, Production, Target IRR, Lease Payment\n",
    "                'Results': ['B4', 'B5', 'B6']  # IRR, NPV, Lease Payment\n",
    "            },\n",
    "            'test_scenarios': [\n",
    "                {'name': 'Default Values', 'inputs': {}},\n",
    "                {'name': 'High Growth', 'inputs': {'B5': 150000, 'B7': 0.05}},\n",
    "                {'name': 'Low IRR Target', 'inputs': {'B13': 0.08}},\n",
    "                {'name': 'High Capex', 'inputs': {'B4': 15000000}}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(f\"🧪 Excel Model Validator initialized for: {self.excel_file_path.name}\")\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry\"\"\"\n",
    "        self._open_excel()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit\"\"\"\n",
    "        self._close_excel()\n",
    "    \n",
    "    def _open_excel(self):\n",
    "        \"\"\"Open Excel application and workbook\"\"\"\n",
    "        try:\n",
    "            self.app = xw.App(visible=False)\n",
    "            self.wb = self.app.books.open(self.excel_file_path)\n",
    "            print(f\"✅ Excel workbook opened for validation\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to open Excel file: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _close_excel(self):\n",
    "        \"\"\"Close Excel application\"\"\"\n",
    "        if self.wb:\n",
    "            self.wb.close()\n",
    "        if self.app:\n",
    "            self.app.quit()\n",
    "    \n",
    "    def _run_test(self, test_func, test_name: str) -> ValidationResult:\n",
    "        \"\"\"Run a single validation test with timing\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = test_func()\n",
    "            if isinstance(result, ValidationResult):\n",
    "                result.execution_time = time.time() - start_time\n",
    "                return result\n",
    "            else:\n",
    "                return ValidationResult(\n",
    "                    test_name=test_name,\n",
    "                    passed=bool(result),\n",
    "                    actual_value=result,\n",
    "                    execution_time=time.time() - start_time\n",
    "                )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=test_name,\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Test failed with error: {str(e)}\",\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "    \n",
    "    def test_model_functionality(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Step 1: Test that optimized model functions identically to original\n",
    "        \"\"\"\n",
    "        print(\"\\n🔧 Step 1: Testing Model Functionality...\")\n",
    "        \n",
    "        functionality_results = {\n",
    "            'macro_tests': {},\n",
    "            'calculation_tests': {},\n",
    "            'formula_integrity': {},\n",
    "            'data_consistency': {}\n",
    "        }\n",
    "        \n",
    "        # Test 1: Macro Functionality\n",
    "        print(\"   Testing macro functionality...\")\n",
    "        for macro_name in self.test_config['required_macros']:\n",
    "            test_result = self._run_test(\n",
    "                lambda m=macro_name: self._test_macro_execution(m),\n",
    "                f\"Macro: {macro_name}\"\n",
    "            )\n",
    "            functionality_results['macro_tests'][macro_name] = test_result.to_dict()\n",
    "            self.validation_results.append(test_result)\n",
    "        \n",
    "        # Test 2: Key Calculation Integrity\n",
    "        print(\"   Testing calculation integrity...\")\n",
    "        calc_result = self._run_test(\n",
    "            self._test_calculation_integrity,\n",
    "            \"Calculation Integrity\"\n",
    "        )\n",
    "        functionality_results['calculation_tests'] = calc_result.to_dict()\n",
    "        self.validation_results.append(calc_result)\n",
    "        \n",
    "        # Test 3: Formula Integrity\n",
    "        print(\"   Testing formula integrity...\")\n",
    "        formula_result = self._run_test(\n",
    "            self._test_formula_integrity,\n",
    "            \"Formula Integrity\"\n",
    "        )\n",
    "        functionality_results['formula_integrity'] = formula_result.to_dict()\n",
    "        self.validation_results.append(formula_result)\n",
    "        \n",
    "        # Test 4: Data Consistency\n",
    "        print(\"   Testing data consistency...\")\n",
    "        data_result = self._run_test(\n",
    "            self._test_data_consistency,\n",
    "            \"Data Consistency\"\n",
    "        )\n",
    "        functionality_results['data_consistency'] = data_result.to_dict()\n",
    "        self.validation_results.append(data_result)\n",
    "        \n",
    "        passed_tests = sum(1 for r in self.validation_results if r.passed)\n",
    "        total_tests = len(self.validation_results)\n",
    "        \n",
    "        print(f\"   ✅ Functionality tests: {passed_tests}/{total_tests} passed\")\n",
    "        \n",
    "        return functionality_results\n",
    "    \n",
    "    def _test_macro_execution(self, macro_name: str) -> ValidationResult:\n",
    "        \"\"\"Test that a macro executes without errors\"\"\"\n",
    "        try:\n",
    "            # Store original values\n",
    "            original_values = self._capture_key_cell_values()\n",
    "            \n",
    "            # Execute macro\n",
    "            self.wb.macro(macro_name)()\n",
    "            \n",
    "            # Check that macro executed (some cells should change or stay valid)\n",
    "            new_values = self._capture_key_cell_values()\n",
    "            \n",
    "            # Verify we get reasonable results\n",
    "            results_valid = self._validate_results_reasonableness(new_values)\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=f\"Macro Execution: {macro_name}\",\n",
    "                passed=results_valid,\n",
    "                actual_value=f\"Macro executed, results valid: {results_valid}\",\n",
    "                message=\"Macro executed successfully\" if results_valid else \"Macro executed but results seem invalid\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=f\"Macro Execution: {macro_name}\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Macro execution failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _test_calculation_integrity(self) -> ValidationResult:\n",
    "        \"\"\"Test that calculations produce consistent results\"\"\"\n",
    "        try:\n",
    "            # Set known inputs and test calculations\n",
    "            inputs = self.wb.sheets['Inputs']\n",
    "            results = self.wb.sheets['Results']\n",
    "            \n",
    "            # Set test values\n",
    "            inputs.range('B4').value = 10000000  # Capex\n",
    "            inputs.range('B5').value = 100000    # Production\n",
    "            inputs.range('B13').value = 0.12     # Target IRR\n",
    "            inputs.range('B14').value = 1200000  # Lease Payment\n",
    "            \n",
    "            # Force recalculation\n",
    "            self.wb.macro('UpdateFinancialModel')()\n",
    "            \n",
    "            # Get calculated results\n",
    "            irr = results.range('B4').value\n",
    "            npv = results.range('B5').value\n",
    "            lease_payment = results.range('B6').value\n",
    "            \n",
    "            # Validate reasonableness\n",
    "            checks = [\n",
    "                ('IRR is numeric', isinstance(irr, (int, float))),\n",
    "                ('IRR is reasonable', 0.01 <= irr <= 1.0 if isinstance(irr, (int, float)) else False),\n",
    "                ('NPV is numeric', isinstance(npv, (int, float))),\n",
    "                ('Lease Payment matches input', abs(lease_payment - 1200000) < 1000 if isinstance(lease_payment, (int, float)) else False)\n",
    "            ]\n",
    "            \n",
    "            all_passed = all(check[1] for check in checks)\n",
    "            failed_checks = [check[0] for check in checks if not check[1]]\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=\"Calculation Integrity\",\n",
    "                passed=all_passed,\n",
    "                actual_value={\n",
    "                    'IRR': irr,\n",
    "                    'NPV': npv,\n",
    "                    'Lease Payment': lease_payment\n",
    "                },\n",
    "                message=f\"Failed checks: {failed_checks}\" if failed_checks else \"All calculations valid\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=\"Calculation Integrity\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Calculation test failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _test_formula_integrity(self) -> ValidationResult:\n",
    "        \"\"\"Test that formulas are intact and not broken\"\"\"\n",
    "        try:\n",
    "            error_cells = []\n",
    "            total_formulas = 0\n",
    "            \n",
    "            # Check all sheets for formula errors\n",
    "            for sheet in self.wb.sheets:\n",
    "                if sheet.used_range:\n",
    "                    for cell in sheet.used_range:\n",
    "                        if cell.formula:\n",
    "                            total_formulas += 1\n",
    "                            value = cell.value\n",
    "                            \n",
    "                            # Check for common Excel errors\n",
    "                            if isinstance(value, str) and value.startswith('#'):\n",
    "                                error_cells.append(f\"{sheet.name}!{cell.address}: {value}\")\n",
    "            \n",
    "            has_errors = len(error_cells) > 0\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=\"Formula Integrity\",\n",
    "                passed=not has_errors,\n",
    "                actual_value=f\"{total_formulas} formulas checked, {len(error_cells)} errors\",\n",
    "                message=f\"Formula errors found: {error_cells[:5]}\" if has_errors else \"All formulas valid\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=\"Formula Integrity\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Formula integrity test failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _test_data_consistency(self) -> ValidationResult:\n",
    "        \"\"\"Test data consistency across related cells\"\"\"\n",
    "        try:\n",
    "            # Test consistency between related values\n",
    "            inputs = self.wb.sheets['Inputs']\n",
    "            results = self.wb.sheets['Results']\n",
    "            \n",
    "            # Get key values\n",
    "            input_lease = inputs.range('B14').value\n",
    "            result_lease = results.range('B6').value\n",
    "            target_irr = inputs.range('B13').value\n",
    "            actual_irr = results.range('B4').value\n",
    "            \n",
    "            consistency_checks = []\n",
    "            \n",
    "            # Check lease payment consistency\n",
    "            if isinstance(input_lease, (int, float)) and isinstance(result_lease, (int, float)):\n",
    "                lease_consistent = abs(input_lease - result_lease) < 1000\n",
    "                consistency_checks.append(('Lease Payment Consistency', lease_consistent))\n",
    "            \n",
    "            # Check IRR reasonableness\n",
    "            if isinstance(actual_irr, (int, float)) and isinstance(target_irr, (int, float)):\n",
    "                irr_reasonable = 0.01 <= actual_irr <= 1.0\n",
    "                consistency_checks.append(('IRR Reasonableness', irr_reasonable))\n",
    "            \n",
    "            all_consistent = all(check[1] for check in consistency_checks)\n",
    "            failed_checks = [check[0] for check in consistency_checks if not check[1]]\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=\"Data Consistency\",\n",
    "                passed=all_consistent,\n",
    "                actual_value={\n",
    "                    'input_lease': input_lease,\n",
    "                    'result_lease': result_lease,\n",
    "                    'target_irr': target_irr,\n",
    "                    'actual_irr': actual_irr\n",
    "                },\n",
    "                message=f\"Failed consistency checks: {failed_checks}\" if failed_checks else \"All data consistent\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=\"Data Consistency\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Data consistency test failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def test_solver_performance(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Step 2: Test solver and optimization performance\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 Step 2: Testing Solver Performance...\")\n",
    "        \n",
    "        solver_results = {\n",
    "            'goal_seek_tests': {},\n",
    "            'convergence_tests': {},\n",
    "            'performance_comparison': {}\n",
    "        }\n",
    "        \n",
    "        # Test Goal Seek functionality\n",
    "        print(\"   Testing Goal Seek performance...\")\n",
    "        goal_seek_result = self._run_test(\n",
    "            self._test_goal_seek_performance,\n",
    "            \"Goal Seek Performance\"\n",
    "        )\n",
    "        solver_results['goal_seek_tests'] = goal_seek_result.to_dict()\n",
    "        self.validation_results.append(goal_seek_result)\n",
    "        \n",
    "        # Test convergence with different scenarios\n",
    "        print(\"   Testing convergence scenarios...\")\n",
    "        convergence_results = {}\n",
    "        \n",
    "        for scenario in self.test_config['test_scenarios']:\n",
    "            scenario_result = self._run_test(\n",
    "                lambda s=scenario: self._test_scenario_convergence(s),\n",
    "                f\"Convergence: {scenario['name']}\"\n",
    "            )\n",
    "            convergence_results[scenario['name']] = scenario_result.to_dict()\n",
    "            self.validation_results.append(scenario_result)\n",
    "        \n",
    "        solver_results['convergence_tests'] = convergence_results\n",
    "        \n",
    "        # Performance comparison with baseline\n",
    "        if self.baseline_data:\n",
    "            print(\"   Comparing with baseline performance...\")\n",
    "            perf_result = self._run_test(\n",
    "                self._test_performance_comparison,\n",
    "                \"Performance Comparison\"\n",
    "            )\n",
    "            solver_results['performance_comparison'] = perf_result.to_dict()\n",
    "            self.validation_results.append(perf_result)\n",
    "        \n",
    "        print(f\"   ✅ Solver tests completed\")\n",
    "        \n",
    "        return solver_results\n",
    "    \n",
    "    def _test_goal_seek_performance(self) -> ValidationResult:\n",
    "        \"\"\"Test Goal Seek performance and accuracy\"\"\"\n",
    "        try:\n",
    "            inputs = self.wb.sheets['Inputs']\n",
    "            results = self.wb.sheets['Results']\n",
    "            \n",
    "            # Set initial conditions\n",
    "            inputs.range('B4').value = 10000000  # Capex\n",
    "            inputs.range('B13').value = 0.12     # Target IRR\n",
    "            inputs.range('B14').value = 1000000  # Initial lease payment\n",
    "            \n",
    "            # Measure Goal Seek performance\n",
    "            start_time = time.time()\n",
    "            \n",
    "            target_cell = results.range('B4')  # IRR\n",
    "            changing_cell = inputs.range('B14')  # Lease Payment\n",
    "            target_value = 0.12\n",
    "            \n",
    "            # Run Goal Seek\n",
    "            target_cell.api.GoalSeek(Goal=target_value, ChangingCell=changing_cell.api)\n",
    "            \n",
    "            goal_seek_time = time.time() - start_time\n",
    "            \n",
    "            # Check results\n",
    "            achieved_irr = target_cell.value\n",
    "            final_lease = changing_cell.value\n",
    "            \n",
    "            # Validate convergence\n",
    "            if isinstance(achieved_irr, (int, float)):\n",
    "                convergence_error = abs(achieved_irr - target_value)\n",
    "                converged = convergence_error < 0.001\n",
    "            else:\n",
    "                converged = False\n",
    "                convergence_error = float('inf')\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=\"Goal Seek Performance\",\n",
    "                passed=converged and goal_seek_time < 10.0,  # Should converge in < 10 seconds\n",
    "                actual_value={\n",
    "                    'execution_time': goal_seek_time,\n",
    "                    'achieved_irr': achieved_irr,\n",
    "                    'target_irr': target_value,\n",
    "                    'convergence_error': convergence_error,\n",
    "                    'final_lease_payment': final_lease\n",
    "                },\n",
    "                message=f\"Goal Seek {'converged' if converged else 'failed'} in {goal_seek_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=\"Goal Seek Performance\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Goal Seek test failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _test_scenario_convergence(self, scenario: Dict) -> ValidationResult:\n",
    "        \"\"\"Test convergence for a specific scenario\"\"\"\n",
    "        try:\n",
    "            inputs = self.wb.sheets['Inputs']\n",
    "            \n",
    "            # Apply scenario inputs\n",
    "            for cell, value in scenario['inputs'].items():\n",
    "                inputs.range(cell).value = value\n",
    "            \n",
    "            # Run solver\n",
    "            start_time = time.time()\n",
    "            self.wb.macro('SolveForLeasePayment')()\n",
    "            solve_time = time.time() - start_time\n",
    "            \n",
    "            # Check convergence\n",
    "            results = self.wb.sheets['Results']\n",
    "            target_irr = inputs.range('B13').value\n",
    "            achieved_irr = results.range('B4').value\n",
    "            \n",
    "            if isinstance(achieved_irr, (int, float)) and isinstance(target_irr, (int, float)):\n",
    "                convergence_error = abs(achieved_irr - target_irr)\n",
    "                converged = convergence_error < 0.001\n",
    "            else:\n",
    "                converged = False\n",
    "                convergence_error = float('inf')\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=f\"Scenario Convergence: {scenario['name']}\",\n",
    "                passed=converged,\n",
    "                actual_value={\n",
    "                    'scenario': scenario['name'],\n",
    "                    'solve_time': solve_time,\n",
    "                    'converged': converged,\n",
    "                    'convergence_error': convergence_error,\n",
    "                    'target_irr': target_irr,\n",
    "                    'achieved_irr': achieved_irr\n",
    "                },\n",
    "                message=f\"Scenario {'converged' if converged else 'failed'} in {solve_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=f\"Scenario Convergence: {scenario['name']}\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Scenario test failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _test_performance_comparison(self) -> ValidationResult:\n",
    "        \"\"\"Compare performance with baseline data\"\"\"\n",
    "        try:\n",
    "            if not self.baseline_data or 'solver_performance' not in self.baseline_data:\n",
    "                return ValidationResult(\n",
    "                    test_name=\"Performance Comparison\",\n",
    "                    passed=True,\n",
    "                    actual_value=\"No baseline data available\",\n",
    "                    message=\"Skipped: No baseline performance data\"\n",
    "                )\n",
    "            \n",
    "            # Run current performance test\n",
    "            start_time = time.time()\n",
    "            self.wb.macro('SolveForLeasePayment')()\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            # Get baseline time\n",
    "            baseline_time = self.baseline_data['solver_performance'].get('goal_seek', {}).get('duration_seconds', 0)\n",
    "            \n",
    "            if baseline_time > 0:\n",
    "                performance_ratio = current_time / baseline_time\n",
    "                acceptable_performance = performance_ratio <= self.test_config['performance_tolerance']\n",
    "            else:\n",
    "                acceptable_performance = True\n",
    "                performance_ratio = 1.0\n",
    "            \n",
    "            return ValidationResult(\n",
    "                test_name=\"Performance Comparison\",\n",
    "                passed=acceptable_performance,\n",
    "                actual_value={\n",
    "                    'current_time': current_time,\n",
    "                    'baseline_time': baseline_time,\n",
    "                    'performance_ratio': performance_ratio,\n",
    "                    'acceptable': acceptable_performance\n",
    "                },\n",
    "                message=f\"Performance {'acceptable' if acceptable_performance else 'degraded'}: {performance_ratio:.1f}x baseline\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                test_name=\"Performance Comparison\",\n",
    "                passed=False,\n",
    "                actual_value=None,\n",
    "                message=f\"Performance comparison failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def measure_optimization_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Step 3: Measure and track optimization metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 Step 3: Measuring Optimization Metrics...\")\n",
    "        \n",
    "        metrics = {\n",
    "            'performance_metrics': {},\n",
    "            'memory_metrics': {},\n",
    "            'calculation_metrics': {},\n",
    "            'file_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(\"   Measuring performance metrics...\")\n",
    "        perf_tests = [\n",
    "            ('Full Recalculation', lambda: self.app.api.Calculate()),\n",
    "            ('Update Model Macro', lambda: self.wb.macro('UpdateFinancialModel')()),\n",
    "            ('Solve Lease Payment', lambda: self.wb.macro('SolveForLeasePayment')())\n",
    "        ]\n",
    "        \n",
    "        for test_name, test_func in perf_tests:\n",
    "            times = []\n",
    "            for _ in range(3):  # 3 runs for average\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    test_func()\n",
    "                    times.append(time.time() - start_time)\n",
    "                except:\n",
    "                    times.append(float('inf'))\n",
    "            \n",
    "            metrics['performance_metrics'][test_name.lower().replace(' ', '_')] = {\n",
    "                'average_time': sum(times) / len(times),\n",
    "                'min_time': min(times),\n",
    "                'max_time': max(times),\n",
    "                'runs': times\n",
    "            }\n",
    "        \n",
    "        # File metrics\n",
    "        file_size = self.excel_file_path.stat().st_size / (1024 * 1024)  # MB\n",
    "        metrics['file_metrics'] = {\n",
    "            'file_size_mb': round(file_size, 2),\n",
    "            'worksheet_count': len(self.wb.sheets)\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ Optimization metrics measured\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _capture_key_cell_values(self) -> Dict:\n",
    "        \"\"\"Capture values from key cells for comparison\"\"\"\n",
    "        values = {}\n",
    "        \n",
    "        try:\n",
    "            for sheet_name, cells in self.test_config['critical_cells'].items():\n",
    "                sheet = self.wb.sheets[sheet_name]\n",
    "                values[sheet_name] = {}\n",
    "                \n",
    "                for cell_addr in cells:\n",
    "                    try:\n",
    "                        values[sheet_name][cell_addr] = sheet.range(cell_addr).value\n",
    "                    except:\n",
    "                        values[sheet_name][cell_addr] = None\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    def _validate_results_reasonableness(self, values: Dict) -> bool:\n",
    "        \"\"\"Check if results are reasonable/valid\"\"\"\n",
    "        try:\n",
    "            results = values.get('Results', {})\n",
    "            \n",
    "            # Check IRR\n",
    "            irr = results.get('B4')\n",
    "            if not isinstance(irr, (int, float)) or not (0.01 <= irr <= 1.0):\n",
    "                return False\n",
    "            \n",
    "            # Check NPV\n",
    "            npv = results.get('B5')\n",
    "            if not isinstance(npv, (int, float)):\n",
    "                return False\n",
    "            \n",
    "            # Check Lease Payment\n",
    "            lease = results.get('B6')\n",
    "            if not isinstance(lease, (int, float)) or lease <= 0:\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def run_full_validation(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Run complete validation suite\n",
    "        \"\"\"\n",
    "        print(f\"🧪 Starting Full Model Validation: {self.excel_file_path.name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run all validation steps\n",
    "        try:\n",
    "            # Step 1: Functionality Tests\n",
    "            functionality_results = self.test_model_functionality()\n",
    "            \n",
    "            # Step 2: Solver Performance Tests\n",
    "            solver_results = self.test_solver_performance()\n",
    "            \n",
    "            # Step 3: Optimization Metrics\n",
    "            optimization_metrics = self.measure_optimization_metrics()\n",
    "            \n",
    "            # Compile results\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            validation_summary = {\n",
    "                'validation_completed': datetime.now().isoformat(),\n",
    "                'total_validation_time': round(total_time, 2),\n",
    "                'excel_file': str(self.excel_file_path),\n",
    "                'test_results': {\n",
    "                    'functionality_tests': functionality_results,\n",
    "                    'solver_tests': solver_results,\n",
    "                    'optimization_metrics': optimization_metrics\n",
    "                },\n",
    "                'summary_stats': {\n",
    "                    'total_tests_run': len(self.validation_results),\n",
    "                    'tests_passed': sum(1 for r in self.validation_results if r.passed),\n",
    "                    'tests_failed': sum(1 for r in self.validation_results if not r.passed),\n",
    "                    'pass_rate': round(sum(1 for r in self.validation_results if r.passed) / max(len(self.validation_results), 1) * 100, 1)\n",
    "                },\n",
    "                'all_test_details': [r.to_dict() for r in self.validation_results]\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n🎉 Validation Complete! Total time: {total_time:.2f}s\")\n",
    "            print(f\"📊 Tests passed: {validation_summary['summary_stats']['tests_passed']}/{validation_summary['summary_stats']['total_tests_run']} ({validation_summary['summary_stats']['pass_rate']}%)\")\n",
    "            \n",
    "            return validation_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Validation failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4256c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for Excel Model Validator\n",
    "    \"\"\"\n",
    "    excel_file = \"project_finance_lease_model.xlsm\"\n",
    "    \n",
    "    if not Path(excel_file).exists():\n",
    "        print(f\"❌ Excel file not found: {excel_file}\")\n",
    "        return\n",
    "    \n",
    "    # Load baseline data if available\n",
    "    baseline_file = Path(excel_file).stem + \"_profiling_results.json\"\n",
    "    baseline_data = None\n",
    "    \n",
    "    if Path(baseline_file).exists():\n",
    "        with open(baseline_file, 'r') as f:\n",
    "            baseline_data = json.load(f).get('analysis_results', {})\n",
    "        print(f\"📂 Loaded baseline data from: {baseline_file}\")\n",
    "    \n",
    "    # Run validation\n",
    "    with ExcelModelValidator(excel_file, baseline_data) as validator:\n",
    "        results = validator.run_full_validation()\n",
    "        \n",
    "        # Save validation results\n",
    "        output_file = Path(excel_file).stem + \"_validation_results.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Validation results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1699cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
